### Does noise is same as outliers? Discuss various noise reduction techniques in Data Mining.

Noise and outliers are not the same. Noise refers to random errors or variance in a measured variable, while outliers are data objects that deviate significantly from the rest of the data and may be generated by a different mechanism.

There are various techniques for reducing noise in data mining. These include binning, regression, clustering, principal component analysis, and attribute subset selection.

Binning involves dividing the data into equal-frequency bins and replacing the values in each bin with a summary statistic, such as the mean or median. Regression techniques can be used to smooth out data by fitting a curve to the data and replacing noisy values with the predicted values from the curve.

Clustering methods can also be used for noise reduction, by grouping similar data objects together and replacing noisy values with the average value of the cluster. Principal component analysis is a dimensionality reduction technique that can also help reduce noise by identifying the most significant features in the data.

Attribute subset selection involves selecting a subset of the most relevant attributes in the data, which can help reduce the effects of noise in the data.

Overall, the choice of noise reduction technique will depend on the specific data set and problem at hand. It is important to carefully consider the characteristics of the data and the goals of the analysis before selecting a technique for noise
 - Source: [72, 581, 126, 580]


### Highlights the different methods used for missing value analysis in Data preprocessing.

      The following methods are used for missing value analysis in data preprocessing:
      1. Ignoring the tuple
      2. Filling in the missing value manually
      3. Using a global constant to fill in the missing value
      4. Using a measure of central tendency for the attribute
      5. Using the attribute mean or median for all samples belonging to the same class
      6. Using the most probable value to fill in the missing value.
 - Source: [88, -25, 120, 92]


### What is Datawarehouse? How it is different from operational databases? Write the features of Data warehouse.

A data warehouse is a subject-oriented, integrated, time-variant, and nonvolatile collection of data in support of management's decision-making process. It is a data repository that is maintained separately from an organization's operational databases and allows for integration of a variety of application systems.

Data warehouse systems are different from operational databases in several ways. First, they are subject-oriented, meaning they are organized around major subjects such as customer, supplier, product, and sales, rather than the day-to-day operations of an organization. Second, they are integrated, meaning they are constructed by integrating multiple heterogeneous sources, such as relational databases, flat files, and online transaction records. Third, they are time-variant, meaning they store data from a historic perspective, typically the past 5-10 years. And finally, they are nonvolatile, meaning they do not require transaction processing, recovery, and concurrency control mechanisms like operational databases do.

The features of a data warehouse include:

1. Subject-oriented: A data warehouse is organized around major subjects, making it easier for decision-makers to access and analyze data related to a specific area.

2. Integrated: Data warehouses integrate data from multiple, heterogeneous sources, ensuring consistency in naming conventions, encoding structures, and attribute measures.

3. Time-
 - Source: [126, 128, 129, 127]


### Differentiate Star and Snowflakes schema.

The star schema is a data model that consists of a large central table (fact table) surrounded by smaller dimension tables, whereas the snowflake schema is a variant of the star schema where some dimension tables are normalized, resulting in a shape similar to a snowflake. The main difference between the two is that the snowflake schema may have more tables due to the normalization process, which can reduce redundancies but can also impact system performance. The star schema is more commonly used in data warehouse design, while the snowflake schema is less popular.
 - Source: [140, 141, 139, 149]


### Explain with suitable example the total ordering and partial ordering of attribute through concept hierarchy.

Total ordering of attributes through concept hierarchy refers to the arrangement of attributes in a hierarchical manner, where each attribute is a subset of the one above it. This means that the values of the higher-level attribute fully encompass the values of the lower-level attribute. For example, in a concept hierarchy for location, "country" would be at the top level, followed by "province/state", "city", and "street" at lower levels. This creates a total order, where each attribute is a more specific subset of the one above it.

Partial ordering of attributes through concept hierarchy refers to the arrangement of attributes in a lattice-like structure, where some attributes may not have a direct relationship with each other. This can occur when there are multiple ways to group or categorize attributes. For example, in a concept hierarchy for time, "day" may be directly below "month", but "week" may be grouped with both "month" and "year". This creates a partial order, where some attributes are grouped together in a lattice rather than a strict hierarchy. 
 - Source: [118, 144, 119, 180]


### What is sampling? What are the advantages of sampling? 2 Differentiate cluster and stratified sampling.

Sampling is the process of selecting a smaller subset of a larger dataset for analysis. It involves randomly selecting data points from the larger dataset to create a representative sample.

The advantages of sampling include:
- It reduces the cost and time required for data analysis as it only involves working with a subset of the data.
- It can provide accurate results if the sample is representative of the larger dataset.
- It allows for progressive refinement of data, meaning the sample can be increased for more precise analysis.

Cluster sampling involves dividing the dataset into mutually disjoint clusters and then randomly selecting clusters to create a sample. Stratified sampling involves dividing the dataset into mutually disjoint strata and then randomly selecting samples from each stratum to create a sample. The main difference between the two is that cluster sampling involves selecting entire clusters while stratified sampling involves selecting samples from each stratum.
 - Source: [109, 110, 108, 444]


### Apply min-max normalization over the given data to scale the data value within [0-2]. (800,1000,1500,2000)

      Applying min-max normalization with new min = 0 and new max = 2, we get the following scaled values:
      v'1 = (800 - 800) * (2 - 0) / (2000 - 800) + 0 = 0
      v'2 = (1000 - 800) * (2 - 0) / (2000 - 800) + 0 = 0.25
      v'3 = (1500 - 800) * (2 - 0) / (2000 - 800) + 0 = 1
      v'4 = (2000 - 800) * (2 - 0) / (2000 - 800) + 0 = 2

      Therefore, the scaled values are 0, 0.25, 1, and 2, respectively.
 - Source: [114, 115, 121, 424]


###  Suppose the 7th Semester Data Mining (DM) mark and Introduction to Machine Learning (IML) mark of 5 students are as follows: Student DM mark IML mark S1 5 11 4 S 3 11 1 0 S4 14 11 12 12 17

S1: DM mark - 5, IML mark - 11
S2: DM mark - 3, IML mark - 11
S3: DM mark - 1, IML mark - 0
S4: DM mark - 14, IML mark - 12
S5: DM mark - 12, IML mark - 17
 - Source: [-35, -7, 606, 36]


### what is the covariance formula used in book, it has n-1 or n in denominator?

The covariance formula used in the book is Cov(A,B) = ∑(ai - ¯A)(bi - ¯B) / n, where n is the number of observations. This formula does not include the n-1 term in the denominator.
 - Source: [98, 96, 424, 97]


### Find Q1, Q2, Q3 for following data distribution and draw box and whisker plot: [2,6,7,8,8,11,12,13,14,15,22,23]

Q1 = 7.5
Q2 = 11.5
Q3 = 14.5

Box and whisker plot:

     0                        5                        10                        15                        20                   
    |------------------------|-----------==============|======================---|------------------------|------------------------|  
 - Source: [50, 49, 52, 555]


### whats is formula to calculate quartile range, Q1, Q2, Q3?

The formula for calculating the quartile range (IQR) is Q3 - Q1. The formula for calculating Q1 is (n+1)/4, where n is the number of data points. The formula for calculating Q2 (the median) is (n+1)/2. The formula for calculating Q3 is 3(n+1)/4.
 - Source: [49, 48, 50, 52]


### should we take avg of 2 values while calculating median or just taken (n+1)/2 if no of observations are even?

When calculating the median, if there is an even number of observations, the average of the two middlemost values should be taken. This is the convention, as shown in Example 2.7 in the given context.
 - Source: [46, 45, 50, 49]


### Compute Jaccard similarity matrix for the given data. (Wrong)

Table
| Patient | Test 1 | Test 2 | Test 3 |
|---------|---------|---------|---------|
| 1 | Positive| Negative| Negative|
| 2 | Negative| Negative| Positive|
| 3 | Negative| Positive| Negative|
| 4 | Positive| Positive| Positive|
| 5 | Negative| Positive| Positive|

| Patient | 1 | 2 | 3 | 4 | 5 |
|---------|---|---|---|---|---|
| 1 | 1 | 0 | 0 | 0 | 0 |
| 2 | 0 | 1 | 0 | 0 | 0 |
| 3 | 0 | 0 | 1 | 0 | 0 |
| 4 | 0 | 0 | 0 | 1 | 0 |
| 5 | 0 | 0 | 0 | 0 | 1 |
 - Source: [77, 71, 72, 67]


